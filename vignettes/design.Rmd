---
title: "dust: Principles and design"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{dust: Principles and design}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  error = FALSE,
  collapse = TRUE,
  comment = "#>",
  fig.width = 7,
  fig.height = 5)
lang_output <- function(x, lang) {
  cat(c(sprintf("```%s", lang), x, "```"), sep = "\n")
}
cc_output <- function(x) lang_output(x, "cc")
r_output <- function(x) lang_output(x, "r")
plain_output <- function(x) lang_output(x, "plain")
set.seed(1)
```

Fundamentally, stochastic Monte Carlo models are extremely simple. Consider a random walk in one dimension - we might write that in base R functions by creating a function that takes a current state `state` and a list of parameters:

```{r}
update_walk <- function(state, pars) {
  rnorm(1, state, pars$sd)
}
```

and then iterating it for 20 steps with:

```{r}
y <- 0
pars <- list(sd = 2)
for (i in 1:20) {
  y <- update_walk(y, pars)
}
```

At the end of this process, the variable `y` contains a new value, corresponding to 20 steps with our stochastic update function.

So why does `dust` apparently require thousands of lines of code to do this?

## Running multiple realisations

### Organisation

It's very rare that one might want to run a single stochastic simulation; normally we want to run a group together.

### Parallelisation

One cannot just draw multiple numbers from a single random number generator at once. That is, given a generator like those built into R, there is no paralel equivalent to

```r
runif(10)
```

that would draw the 10 numbers in parallel rather than in series. When drawing a random number there is a "[side effect](https://en.wikipedia.org/wiki/Side_effect_(computer_science))" of updating the random number state. That is because the random number stream is *also* a Markov chain!

As such it makes sense (to us at least) to store the state of each stream's random number generator separately, so if we have `n` particles within a `dust` object we have `n` separate streams, and we might think of the model state as being the state that is declared by the user as a vector of doubles alongside the random number state. During each model step, the model state is updated and so is the random number state.

This might seem wasteful, and if we used the [Mersenne Twister](https://en.wikipedia.org/wiki/Mersenne_Twister) it would be to some degree as each particle would require 2560 bytes of additional state. In contrast the [xoshiro](http://prng.di.unimi.it/) generators that we 256 bytes of state; the same as 4 double-precision floating point numbers. So for any nontrivial simulation it's not a very large overhead.

Setting the seed for these runs is not trivial, particularly as the number of simultaneous particles increase. If you've used random numbers with the [future](https://cran.r-project.org/package=future) package you may have seen it raise a warning if you do not configure it to use a "L'Ecuyer-CMRG" which adapts R's native random number seeds to be safe in parallel.

The reason for this that if differnet streams start from seeds that are set via poor heuristics (e.g., system time) they might be exactly the same. If they were set randomly, then they might collide (see [John Cook's description of the birthday paradox here](https://www.johndcook.com/blog/2016/01/29/random-number-generator-seed-mistakes/)) and if they are picked sequentially there's no guarantee that these streams might not be correlated.

Ideally we want a similar set of properties to R's `set.seed` method; the user provides an arbitrary integer and we seed _all_ the random number streams using this in a way that is reproducible and also statistically robust. We also want the streams to be reproducible even when the number of particles changes, for particle indices that are shared. To do this we take two steps:

* First, we seed the first stream using the `splitmix64` RNG, following the xoshiro docs. This ensures that the resulting xoshiro random number seed does not contain too many zeros
* Then, for each subsequent chain we take a "jump" in the sequence. This is a special move implemented by the RNG that is equivent to 2^128 draws (about 10^38).

With this setup we are free to parallelise the system as each realisation is completely independent of each other; the problem has become "[embarassingly parallel](https://en.wikipedia.org/wiki/Embarrassingly_parallel)". In practice we do this using [OpenMP](https://www.openmp.org/) where available as this is well supported from R and gracefully falls back on serial operation where not available.  See `dust::dust_openmp_support` for information on your system's OpenMP configuration as seen by R:

```{r}
dust::dust_openmp_support()
```

As the number of threads changes, the results will not change; the same calculations will be carried out and the same random numbers drawn. The number of threads used can even be changed for a model while it is running if the computational resources available change during a model run, using the `$set_n_threads()` method.

The properties of the random number generator are discussed further in `vignette("rng")`.

## Efficient running

A general rule-of-thumb is to avoid unneeded allocations in tight loops; with this sort of stochastic iteration everything is a tight loop! However, we've reduced the problem scope to just providing an update method and if that does not allocate then the whole thing runs in fixed space without having to worry.

## Efficient state handling

For nontrivial systems, we often want to record a subset of states - potentially a very small fraction of the total states computed. For example, in our [sircovid](https://mrc-ide.github.io/sircovid/) model we track several thousand states (representing populations in various stages of disease transmission, in different ages, with different vaccination status etc), but most of the time we only need to report on a few tens of these in order to fit to data or to examine key outputs.

Reducing the number of state variables returned at different points in the process has several advantages

* Saving space: if you run a model with 2000 states, and 1000 replicates and record trajectories over 100 timesteps, that represents 100 million double precision numbers, or 1.6 GB of memory or disk (before compression). These are not unrealistic numbers, but would make even a simple sensitivity analysis all-but impossible to work with.
* Saving time: copying data around is surprisingly slow

To enable this, you can restrict the state retured by most methods; some by default and others when you call them.

* The `$run()` and `$simulate()` methods move the system fowards in time and returns the state at that point; it uses an index set into the object with `$set_index()`. The intention here is that these would be repeatedly called and so we validate the index once and use it over and over.
* The `$state()` method returns the model state and accepts an argument `index` as the state to return

In both cases, if `index` was named then the returned state carries these names as its rownames.

The ordering of the state is important; we always have dimensions that will contain

1. the states within a single particle
2. the particles within a time (may be several dimensions; see `vignette("multi")`)
3. the time dimension if using

This is to minimise churn during writing, and to help with concatenation. Multiple particles data is stored consecutively and read and written in order. Each time step is written at once. And you can append states from different times easily. The `aperm()` function will be useful for reshaping this output to a different dimension order if you require one.

In order to pull all of this off, we allocate all our memory up front, in C++ and pass back to R a "pointer" to this memory, which will live for as long as your model object. This means that even if your model requires GBs of memory to run, it is never copied back and forth into R (where it would be subject to R's [copy-on-write semantics](https://en.wikipedia.org/wiki/Copy-on-write) but instead accessed only when needed, and written to in place following C++ reference semantics.

## Useful verbs

We try and provide verbs that are useful, given that the model presents a largely opaque pointer to model state. These are driven by our needs for running a particle filter.

Normally we have in the object several things:

* the random number state: this is effectively a matrix of 64-bit integers
* the model state: this is effectively a matrix of floating point numbers, typically `double`s
* the model parameters: this is specific to the model in question and these are likely shared across multiple particles). This is presented to all particles within a group as an immutable pointer, which allows safe simultaneous access by multiple threads
* the model _internal state_: this is also specific to the model and dust has no control over this. Because each particle has its own state they can safely write to it when running parallel (compare with parameters). Typically this space is *allocated* at model construction. For models that come from [odin.dust](https://mrc-ide.github.io/odin.dust/) this is used as "scratch" space that we write to during an update but which does not conceptually persist between steps.

There is one subtlety about internal state: we assume that the state entirely specifies a model in a Markov process and we don't guarantee that models with mutable internal state that is not conceptually discarded each iteration. This is because if we reorder particles what we really do is reorder the *state vector* and not this internal state. This prevents implementing things like models with "delays" in the current design. We may relax constraint this if it is needed.

The sorts of verbs that we need given this include:

* Setting model state (`$set_state`) - leaves RNG state and parameters untouched but replaces model state for all particles. This is useful for model initialisation and for performing arbitrary model state changes.
* Running the model up to a time point (`$run`) - runs the model's `update` method as many times as required to reach the new time point, returning the model state at this time point. This is useful where you might want to change the system at this time point, then continue.
* Running the model and collecting history (`$simulate`) - as for `$run` but also collects partial state at a number of times along the way. This always has one more dimension than `$run` (being time) and the two functions coexist so that dimensionality is easy to program against.
* Reset particles and their parameters (`$reset` and `$set_pars`) which

In addition, we have more specific methods oriented towards [particle filtering](https://en.wikipedia.org/wiki/Particle_filter)

* Reordering the particles (`$reorder`) - shuffles particle state among particles within a parameter set. This is useful for implemeting a resampling algorithms and updates only the state (as for `$set_state`, leaving RNG state and internal state untouched)
* Resampling particles according to some weight vector (`$resample`) which implements a bootstrap sampling algorithm on top of `$reorder`
* Run a bootstrap particle filter (`$filter`) which implements a bootstrap particle filter using the above methods, in the case where the model provides a compare function. This is likely to be a bit low level for direct use, and is better approached via the interface in [mcstate](https://mrc-ide.github.io/mcstate/)

## A compilation target

The most esoteric design of dust is to make it convenient to use as a target for other programs. We use the package primarily as a target for models written in [`odin`](https://mrc-ide.github.io/odin/) via [odin.dust](https://mrc-ide.github.io/odin.dust/). This allows the user to write models at a very high level, describing the updates between steps. The random walk example at the beginning of this document might be implemented as

```r
sd <- user()              # user-provided standard deviation
initial(y) <- 0           # starting point of the simulation
update(y) <- runif(y, sd) # take random step each time step
```

which will compile a dust model:

```c++
// [[dust::class(odin)]]
// [[dust::param(sd, has_default = FALSE, default_value = NULL, rank = 0, min = -Inf, max = Inf, integer = FALSE)]]
class odin {
public:
  typedef double real_t;
  typedef dust::no_data data_t;
  struct shared_t {
    real_t initial_y;
    real_t sd;
  };
  struct internal_t {
  };
  odin(const dust::pars_t<odin>& pars) :
    shared(pars.shared), internal(pars.internal) {
  }
  size_t size() {
    return 1;
  }
  std::vector<real_t> initial(size_t step) {
    std::vector<real_t> state(1);
    state[0] = shared->initial_y;
    return state;
  }
  void update(size_t step, const real_t * state, dust::rng_state_t<real_t>& rng_state, real_t * state_next) {
    const real_t y = state[0];
    state_next[0] = dust::distr::runif(rng_state, y, shared->sd);
  }
private:
  std::shared_ptr<const shared_t> shared;
  internal_t internal;
};

// ...
template<>
dust::pars_t<odin> dust_pars<odin>(cpp11::list user) {
  typedef typename odin::real_t real_t;
  auto shared = std::make_shared<odin::shared_t>();
  odin::internal_t internal;
  shared->initial_y = 0;
  shared->sd = NA_REAL;
  shared->sd = user_get_scalar<real_t>(user, "sd", shared->sd, NA_REAL, NA_REAL);
  return dust::pars_t<odin>(shared, internal);
}
```

We have designed these two systems to play well together so the user can write models at a very high level and generate code that then works well within this framework and efficiently run in parallel. In [sircovid](https://mrc-ide.github.io/sircovid/) this is used in a model with hundreds of logical compartments each of which may be structured, but the interface at the R level remains the same as for the toy models used in the documentation here.
